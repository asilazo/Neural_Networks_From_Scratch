{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Networks_from_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-d9v0_bsdbB"
      },
      "source": [
        "Some attempt to implement NN from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEiLfCdDrfSH"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMWHpZfusobg"
      },
      "source": [
        "# create a Neuron class\n",
        "\n",
        "class Neuron(object):\n",
        "  \"\"\"\n",
        "  A simple feed-forward artificial neuron.\n",
        "    Args:\n",
        "      num_inputs (int): The input vector size / number of input values.\n",
        "      activation_fn (callable): The activation function.\n",
        "    Attributes:\n",
        "      W (ndarray): The weight values for each input.\n",
        "      b (float): The bias value, added to the weighted sum.\n",
        "      activation_fn (callable): The activation function.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_inputs, activation_fn):\n",
        "    super().__init__()\n",
        "    # Randomly initializing the weight vector and bias value:\n",
        "    self.W = np.random.rand(num_inputs)\n",
        "    self.b = np.random.rand(1)\n",
        "    self.activation_fn = activation_fn\n",
        "  \n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward the input signal through the neuron.\n",
        "    \n",
        "    \"\"\"\n",
        "    z = np.dot(x, self.W) + self.b\n",
        "    return self.activation_fn(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaTcliB_e4c2",
        "outputId": "35bc7622-a266-4fde-a174-5c98bab3d795"
      },
      "source": [
        "# Fixing the random number generator's seed, for reproducible results:\n",
        "np.random.seed(42)\n",
        "\n",
        "# Random input column array of 3 values (shape = `(1, 3)`)\n",
        "x = np.random.rand(3).reshape(1,3)\n",
        "#print(x)\n",
        "## should print out [[0.37454012 0.95071431 0.73199394]]\n",
        "\n",
        "print(f\"The size of x is: {x.size}\")\n",
        "print(\"--------------------\")\n",
        "\n",
        "# Instantiating a Perceptron (simple neuron with step function):\n",
        "step_fn = lambda y: o if y <= 0 else 1\n",
        "\n",
        "perceptron = Neuron(num_inputs=x.size, activation_fn=step_fn)\n",
        "\n",
        "out = perceptron.forward(x)\n",
        "print(f\"Perceptron output is:{out}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of x is: 3\n",
            "--------------------\n",
            "Perceptron output is:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3GNA-bMjhDT"
      },
      "source": [
        "**Layering neurons together**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdmSDz0bjcGu"
      },
      "source": [
        "class FullyConnectedLayer(object):\n",
        "  \"\"\"\n",
        "  A simple fully-connected NN layer.\n",
        "  Args:\n",
        "      num_inputs (int): The input vector size/number of input values.\n",
        "      layer_size (int): The output vector size/number of neurons.\n",
        "      activation_fn (callable): The activation function for this layer.\n",
        "  Attributes:\n",
        "      W (ndarray): The weight values for each input.\n",
        "      b (ndarray): The bias value, added to the weighted sum.\n",
        "      size (int): The layer size/number of neurons.\n",
        "      activation_fn (callable): The neurons' activation function.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, num_inputs, layer_size, activation_fn):\n",
        "    super().__init__()\n",
        "    # Randomly initializing the parameters (using a normal distribution)\n",
        "    self.W = np.random.standard_normal((num_inputs, layer_size))\n",
        "    self.b = np.random.standard_normal(layer_size)\n",
        "    self.size = layer_size\n",
        "    self.activation_fn = activation_fn\n",
        "  \n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward the input signal through the layer.\n",
        "    \n",
        "    \"\"\"\n",
        "    z =np.dot(x, self.W) + self.b\n",
        "    return self.activation_fn(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_dvGw5qr-TW"
      },
      "source": [
        "We just have to change the dimensionality of some of the variables in order to reflect the\n",
        "multiplicity of neurons inside a layer. With this implementation, our layer can even process\n",
        "several inputs at once! Passing a single column vector x (of shape 1 × s with s number of\n",
        "values in x) or a stack of column vectors (of shape n × s with n number of samples) does not\n",
        "change anything with regard to our matrix calculations, and our layer will correctly output\n",
        "the stacked results (assuming b is added to each row):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkkrhblhoJgZ",
        "outputId": "8a2b5800-9112-4731-a0cc-6cc2919866ab"
      },
      "source": [
        "np.random.seed(42)\n",
        "# Random input column-vectors of 2 values (shape = `(1, 2)`):\n",
        "\n",
        "x1 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n",
        "print(x1)\n",
        "\n",
        "x2 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n",
        "print(x2)\n",
        "\n",
        "relu_fn = lambda y: np.maximum(y, 0) # Defining our relu activation function\n",
        "\n",
        "layer = FullyConnectedLayer(2, 3, relu_fn)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.25091976  0.90142861]]\n",
            "[[0.46398788 0.19731697]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucIVShfjr0Fe"
      },
      "source": [
        "A stack of input data is commonly called a batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8EdNsd0q8ev",
        "outputId": "47e9bfc6-f017-44ca-89d4-c4cc9220f416"
      },
      "source": [
        "# Our layer can process x1 and x2 separately\n",
        "out1 = layer.forward(x1)\n",
        "print(f\"Output for x1: {out1}\")\n",
        "\n",
        "out2 = layer.forward(x2)\n",
        "print(f\"Output for x2: {out2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output for x1: [[0.28712364 0.         0.33478571]]\n",
            "Output for x2: [[0.         0.         1.08175419]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhUjTpB9tEn0"
      },
      "source": [
        "**Applying our network to classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_IuuKQftQX6"
      },
      "source": [
        "Classifying images of handwritten digits (that is, recognizing whether an image contains a\n",
        "0 or a 1 and so on) is a historical problem in computer vision. The Modified National\n",
        "Institute of Standards and Technology (MNIST) dataset (http:/​/​yann.​lecun.​com/​exdb/\n",
        "mnist/​), which contains 70,000 grayscale images (28 × 28 pixels) of such digits, has been\n",
        "used as a reference over the years so that people can test their methods for this recognition\n",
        "task (Yann LeCun and Corinna Cortes hold all copyrights for this dataset)\n",
        "\n",
        "For digit classification, what we want is a network that takes one of these images as input\n",
        "and returns an output vector expressing how strongly the network believes the image\n",
        "corresponds to each class. The input vector has 28 × 28 = 784 values, while the output has 10\n",
        "values (for the 10 different digits, from 0 to 9). In-between all of this, it is up to us to define\n",
        "the number of hidden layers and their sizes. To predict the class of an image, it is then just a\n",
        "matter of forwarding the image vector through the network, collecting the output, and returning\n",
        "the class with the highest belief score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNYPHgDFtG_N",
        "outputId": "51b4dea3-7de5-483c-eb68-0d7278e1387a"
      },
      "source": [
        "!pip3 install mnist\n",
        "\n",
        "import mnist\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mnist\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.19.5)\n",
            "Installing collected packages: mnist\n",
            "Successfully installed mnist-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "o-8ZVuDsuDTQ",
        "outputId": "a6aaafbc-c881-4318-b2d6-ed6e5d545784"
      },
      "source": [
        "# Loading the training and testing data:\n",
        "\n",
        "X_train, y_train = mnist.train_images, mnist.train_labels\n",
        "X_test, y_test = mnist.test_images, mnist.test_labels\n",
        "\n",
        "num_classes = 10 # classes are the digits from 0 to 9\n",
        "\n",
        "print(X_test.shape)\n",
        "\n",
        "# We transform the images into column vectors (as inputs for our NN):\n",
        "#X_train, X_test = X_train.reshape(-1, 28*28), X_test.reshape(-1,28*28)\n",
        "\n",
        "# We \"one-hot\" the labels (as targets for our NN), for instance, transform\n",
        "# label `4` into vector `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`:\n",
        "#y_train = np.eye(num_classes)[y_train]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5d871ba355f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# classes are the digits from 0 to 9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# We transform the images into column vectors (as inputs for our NN):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}